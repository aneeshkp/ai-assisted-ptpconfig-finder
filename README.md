# ai-assisted-ptpconfig-finder



use Ollama instead of llama-cpp for local LLM responses!


    Install Ollama:

curl -fsSL https://ollama.com/install.sh | sh

    Pull a model (e.g., Mistral):

ollama pull mistral

    Run your script:

python ptpconfig-assists-ollama.py


LLaMA 2 model via Ollama instead of Mistral.

âœ… Make sure to pull the model first:

ollama pull llama2
